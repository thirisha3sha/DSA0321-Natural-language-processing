#25.	Utilize the GPT-3 model to generate text based on a given prompt. Make sure to install the OpenAI GPT-3 library in python implementation.
from transformers import pipeline
generator = pipeline('text-generation', model='gpt2', framework='pt')
def generate_text(prompt, max_length=100):
    response = generator(prompt, max_length=max_length, num_return_sequences=1)
    return response[0]['generated_text']
prompt = "Do you know India"
generated_text = generate_text(prompt)
print(f"Generated Text: {generated_text}")
#output:
config.json: 100%
 665/665 [00:00<00:00, 11.7kB/s]
model.safetensors: 100%
 548M/548M [00:06<00:00, 74.7MB/s]
generation_config.json: 100%
 124/124 [00:00<00:00, 2.96kB/s]
tokenizer_config.json: 100%
 26.0/26.0 [00:00<00:00, 628B/s]
vocab.json: 100%
 1.04M/1.04M [00:00<00:00, 5.08MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 5.74MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 10.3MB/s]
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generated Text: Do you know India, if these people feel that India or any other country has taken unfair advantage of them?" she asked.

The question sparked uproar due to allegations of corruption which took place around the world and the allegations against her in several media reports.
